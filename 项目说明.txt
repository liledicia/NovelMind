==================================================================
                   晋江文学城小说数据采集器
                      NovelMind Scrawl
==================================================================

【项目简介】
这是一个用于采集晋江文学城小说数据的Python爬虫项目。
可以自动抓取排行榜上的热门小说信息，并保存到本地数据库。

==================================================================
【主要功能】
==================================================================

1. 排行榜数据采集
   - 自动爬取晋江文学城收藏榜
   - 支持多页翻页采集
   - 默认采集150本小说（可自行调整target_count参数）

2. 小说详细信息抓取
   包含以下信息：
   ✓ 基本信息：书名、作者、简介、标签
   ✓ 角色信息：主角、配角、其它角色
   ✓ 作品属性：文章类型、作品视角、所属系列、连载状态
   ✓ 文章数据：全文字数、章节总数、最后更新时间
   ✓ 出版信息：版权转化状态、签约状态
   ✓ 统计数据：
     - 总书评数
     - 当前被收藏数
     - 营养液数
     - 文章积分
     - 非V章节总点击数

3. 数据持久化存储
   - 使用SQLite数据库存储（jinjiang_novels.db）
   - 两张数据表结构：

     book 表（小说基本信息）
     - book_id: 小说ID（主键）
     - title: 书名
     - author: 作者
     - intro: 简介
     - tags: 标签
     - main_chars: 主角
     - support_chars: 配角
     - other_info: 其它信息
     - category: 文章类型
     - perspective: 作品视角
     - series: 所属系列
     - status: 连载状态
     - word_count: 字数
     - publish_status: 出版状态
     - sign_status: 签约状态
     - last_update_time: 最后更新时间
     - chapter_count: 章节数

     stats 表（统计数据，可记录历史变化）
     - id: 自增主键
     - book_id: 关联小说ID
     - date: 采集日期
     - review_count: 书评数
     - favorite_count: 收藏数
     - nutrient_count: 营养液数
     - total_click_count: 点击数
     - score: 积分
     - chapter_count: 章节数

==================================================================
【技术特点】
==================================================================

✓ 编码处理：自动识别并正确解析GBK/GB18030编码
✓ 速率控制：智能节流，分页间隔1-2秒，详情页间隔0.5-1秒
✓ 容错机制：网络异常自动跳过，不影响整体采集
✓ 断点续爬：使用INSERT OR REPLACE，支持重复运行更新数据
✓ 数据完整：同时采集静态信息和动态统计数据

==================================================================
【依赖环境】
==================================================================

Python版本：Python 3.6+

第三方库：
- requests：HTTP请求库
- beautifulsoup4：HTML解析库
- lxml：高性能解析器

安装命令：
pip install requests beautifulsoup4 lxml

==================================================================
【使用方法】
==================================================================

1. 安装依赖
   pip install requests beautifulsoup4 lxml

2. 运行脚本
   python NovelMindScrawl.py

3. 等待采集完成
   - 脚本会显示采集进度
   - 预计耗时：约2-5分钟（取决于网络速度）

4. 查看数据
   - 数据保存在 jinjiang_novels.db 文件中
   - 可使用SQLite客户端查看（如DB Browser for SQLite）

==================================================================
【参数调整】
==================================================================

可在代码中修改以下参数：

target_count = 150
  # 目标采集小说数量，建议100-200之间

time.sleep(random.uniform(1.0, 2.0))
  # 分页请求间隔，单位：秒

time.sleep(random.uniform(0.5, 1.0))
  # 详情页请求间隔，单位：秒

==================================================================
【注意事项】
==================================================================

⚠ 请遵守网站robots.txt规则
⚠ 请勿过于频繁请求，避免对服务器造成压力
⚠ 仅供学习研究使用，请勿用于商业用途
⚠ 爬取的数据请尊重原作者版权
⚠ 运行过程中请保持网络连接稳定

==================================================================
【输出文件】
==================================================================

jinjiang_novels.db
  SQLite数据库文件，包含所有采集的小说数据

==================================================================
【常见问题】
==================================================================

Q: 为什么有些小说信息不完整？
A: 可能是该小说详情页结构特殊，或者网络请求失败导致。

Q: 可以采集更多小说吗？
A: 可以修改target_count参数，但请注意控制速率，避免被封IP。

Q: 数据库可以重复运行更新吗？
A: 可以，脚本使用INSERT OR REPLACE，会自动更新已存在的记录。


Q: 如何导出数据为Excel或CSV？
A: 可以使用SQLite客户端工具导出，或编写Python脚本读取数据库后导出。

==================================================================
项目创建时间：2026-01-05
Python版本：3.14
==================================================================
